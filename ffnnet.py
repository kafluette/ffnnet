
__docformat__ = 'restructedtext en'

import numpy
import numpy.linalg
import numpy.random
import theano
import theano.tensor as T
from scipy.linalg import hadamard
from scipy.special import gamma

from logistic_sgd import LogisticRegression

from theano_util import *


class HiddenLayer(object):
    def __init__(self, layer_no, num_layers, rng, input, n_in, n_out, d,
                 W=None, b=None, S=None, G=None, B=None):
        self.input = input

        dbg_name = lambda s: s + '_l' + str(layer_no)

        if W is None:
            # initialize using IWI method
            # (https://www.researchgate.net/publication/262678356_Interval_based_Weight_Initialization_Method_for_Sigmoidal_Feedforward_Artificial_Neural_Networks)
            W_values = numpy.asarray(
                rng.uniform(
                    low=(2.0 * layer_no - 1.0) / (num_layers - 1.0),
                    high=(2.0 * layer_no + 1.0) / (num_layers - 1.0),
                    size=(n_in, n_out)
                ),
                dtype=theano.config.floatX
            )
            W = theano.shared(value=W_values, name=dbg_name('W'), borrow=True)

        if b is None:
            b_values = numpy.zeros((n_out,), dtype=theano.config.floatX)
            b = theano.shared(value=b_values, name=dbg_name('b'), borrow=True)

        if G is None:
            diag_values = numpy.asarray(rng.normal(0, 1, size=d))
            G_values = numpy.zeros((d, d))
            for i in xrange(d):
                G_values[i, i] = diag_values[i]
            G = theano.shared(value=G_values, name=dbg_name('G'), borrow=True)
        if B is None:
            diag_values = rng.randint(0, 1, size=d)
            B_values = numpy.zeros((d, d))
            for i in xrange(d):
                B_values[i, i] = diag_values[i]
            B = theano.shared(value=B_values, name=dbg_name('B'), borrow=True)
        if S is None:
            S_values = numpy.zeros((d, d))
            for i in xrange(d):
                s_i = ((2.0 * numpy.pi) ** (-d / 2.0)) * (1.0 / ((numpy.pi ** (d / 2.0)) / gamma(100)))  # gamma((d / 2.0) + 1.0)))
                S_values[i, i] = s_i * (numpy.linalg.norm(G.get_value(borrow=True), ord='fro') ** (-1.0 / 2.0))
            S = theano.shared(value=S_values, name=dbg_name('S'), borrow=True)

        self.W = W
        self.b = b

        # FFnnet params
        self.S = S
        self.G = G
        self.B = B

        lin_output = T.dot(input, self.W) + self.b
        self.output = T.nnet.nnet.sigmoid(lin_output)

        # parameters of the model
        self.params = [self.W, self.b, self.S, self.G, self.B]


class MLP(object):
    def __init__(self, rng, input, n_in, n_layers, n_nodes, n_out, d, output_clz=LogisticRegression):
        self.input = input
        self.n_layers = n_layers
        self.n_nodes = n_nodes
        self.n_in = n_in
        self.n_out = n_out
        self.d = d

        # generate ffnnet parameters perm_matrix (random permutation matrix) and H (hadamard matrix)
        H_values = hadamard(d, dtype=numpy.int)
        H = theano.shared(value=H_values, name='H', borrow=True)
        self.H = H

        perm_matrix_values = numpy.identity(d)  # generated by shuffling the columns of the dxd identity matrix
        numpy.random.shuffle(numpy.transpose(perm_matrix_values))
        perm_matrix = theano.shared(value=perm_matrix_values, name='PI', borrow=True)
        self.perm_matrix = perm_matrix

        # generate the first hidden layer, taking as inputs the input nodes
        self.hiddenLayers = []
        self.hiddenLayers.append(HiddenLayer(
            layer_no=0 + 1,  # IWI indexing starts at 1
            num_layers=n_layers,
            rng=rng,
            input=input,
            n_in=n_in,
            n_out=n_nodes,
            d=d
        ))

        # generate the rest of the hidden layers, taking as inputs the previous layer's output
        for l in xrange(1, n_layers):
            self.hiddenLayers.append(HiddenLayer(
                layer_no=l + 1,
                num_layers=n_layers,
                rng=rng,
                input=self.hiddenLayers[l - 1].output,
                n_in=n_in,
                n_out=n_nodes,
                d=d,
            ))

        # The output layer gets as input the units of the last hidden layer
        self.outputLayer = output_clz(
            input=self.hiddenLayers[n_layers - 1].output,
            n_in=n_nodes,
            n_out=n_out
        )

        # L1 norm ; one regularization option is to enforce L1 norm to be small
        self.L1 = (
            sum([abs(self.hiddenLayers[i].W).sum() for i in xrange(n_layers)])
            + abs(self.outputLayer.W).sum()
        )

        # square of L2 norm ; one regularization option is to enforce square of L2 norm to be small
        self.L2_sqr = (
            sum([self.hiddenLayers[i].W ** 2 for i in xrange(n_layers)])
            + (self.outputLayer.W ** 2).sum()
        )

        # number of errors of the MLP
        self.errors = self.outputLayer.errors

        # the parameters of the model are the parameters of the layers it is made out of (hiddens + output)
        self.params = []
        for i in xrange(n_layers):
            self.params = self.params + self.hiddenLayers[i].params
        self.params = self.params #+ self.outputLayer.params

    def cross_entropy_cost(self, y):
        sigma = 1.0
        def f(l):
            return T.dot(self.hiddenLayers[l].W, self.hiddenLayers[l].W)#T.imag(phi(l, 1)(self.hiddenLayers[l-1].output)))
        def phi(l, j):
            S = self.hiddenLayers[l].S
            G = self.hiddenLayers[l].G
            B = self.hiddenLayers[l].B
            PI = self.perm_matrix
            H = self.H
            d = self.d
            m = self.n_nodes
            return lambda h: 1/T.sqrt(m) * T.exp(1j*((1/sigma*T.sqrt(d)) * S * H * G * PI * H * B * h))
        def h(l, j):
            return T.nnet.sigmoid(T.dot(self.hiddenLayers[l].W, T.imag(phi(l-1, j)(self.hiddenLayers[l-1].output)))) \
                if l > 0 else T.nnet.sigmoid(T.dot(self.hiddenLayers[l].W, T.imag(phi(l-1, j)(self.input))))
        result, updates = theano.scan(
                    fn=lambda n: f(1),
        #            fn=lambda n: y[n] * T.log(T.nnet.sigmoid(f(self.n_layers-1))) + (1 - y[n])*T.log(1 - T.nnet.sigmoid(f(self.n_layers-1))),
                    sequences=[T.arange(10)])
        return -result.sum()

